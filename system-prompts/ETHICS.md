# Ethical Guidelines - AgiTerminal System Prompts Library

**Version**: 1.0
**Effective Date**: 2026-02-05

---

## Table of Contents

- [Our Mission](#our-mission)
- [Core Ethical Commitments](#core-ethical-commitments)
- [Responsible Use](#responsible-use)
- [Legal Compliance](#legal-compliance)
- [Educational Purpose](#educational-purpose)
- [Transparency Principles](#transparency-principles)
- [Contributor Responsibilities](#contributor-responsibilities)
- [Community Standards](#community-standards)

---

## Our Mission

**AgiTerminal's AI System Prompts Library** exists to:
- Promote understanding of AI system architectures
- Support AI safety research through transparency
- Educate developers about prompt engineering
- Enable comparative analysis across AI providers

We believe that understanding how AI systems work is essential for:
- Building safer AI systems
- Improving alignment research
- Fostering informed public discourse
- Developing effective safety measures

---

## Core Ethical Commitments

### 1. Legal Compliance

**We commit to:**
- ✅ Only collect from public, legal sources
- ✅ Respect terms of service
- ✅ Comply with all applicable laws
- ✅ Honor copyright and licensing
- ✅ Never enable unauthorized access

**This means:**
- No extraction from private APIs without permission
- No circumvention of security measures
- No redistribution of proprietary content
- No collection of personal user data

### 2. Educational Purpose

**We commit to:**
- ✅ Prioritize research and education
- ✅ Support AI safety initiatives
- ✅ Enable academic study
- ✅ Promote responsible AI development

**This means:**
- All content serves educational goals
- Research use is clearly documented
- Academic partnerships are encouraged
- Safety-critical findings are highlighted

### 3. Transparency

**We commit to:**
- ✅ Full source attribution
- ✅ Clear documentation of methods
- ✅ Honest about limitations
- ✅ Open about uncertainty

**This means:**
- Every prompt includes source URL
- Extraction methods are documented
- Confidence levels are communicated
- Uncertainty is acknowledged

### 4. Responsibility

**We commit to:**
- ✅ Consider potential misuse
- ✅ Include safety warnings
- ✅ Monitor for harms
- ✅ Respond to concerns

**This means:**
- Active harm prevention measures
- Community guidelines enforced
- Responsive to reported issues
- Continuous ethical review

---

## Responsible Use

### Appropriate Uses of This Repository

✅ **Encouraged**:
- Academic research on AI safety
- Comparative analysis of AI systems
- Educational courses on AI architecture
- Safety engineering case studies
- Alignment research
- Journalism and public education
- Prompt engineering learning
- Transparency advocacy

⚠️ **Allowed with Caution**:
- Commercial analysis (if legal)
- Competitive intelligence (if ethical)
- Product planning (if properly attributed)

❌ **Not Acceptable**:
- Violating terms of service
- Security circumvention
- Harassment or abuse
- Disinformation campaigns
- Unauthorized surveillance
- Malicious automation
- Academic dishonesty
- False attribution

### Specific Guidelines

#### For Researchers

**Do**:
- Cite this repository properly
- Validate findings independently
- Consider ethical implications
- Share safety-relevant discoveries
- Respect provider policies

**Don't**:
- Use prompts to bypass safety measures
- Apply learning to harmful applications
- Misrepresent findings
- Ignore provider concerns

#### For Developers

**Do**:
- Learn from prompt patterns
- Apply insights to safer systems
- Credit techniques used
- Contribute back improvements

**Don't**:
- Copy prompts without understanding
- Use for harmful applications
- Claim them as your own
- Enable misuse by others

#### For Journalists

**Do**:
- Use for factual reporting
- Verify sources independently
- Provide context on limitations
- Highlight safety implications

**Don't**:
- Sensationalize findings
- Mislead about authenticity
- Omit ethical considerations
- Encourage misuse

---

## Legal Compliance

### Copyright Status

Most system prompts exist in a legal gray area:

- **Collection-specific formatting**: © AgiTerminal (MIT License)
- **Prompt content**: Varies by source
- **Analysis and commentary**: © AgiTerminal
- **Compilation**: © AgiTerminal (database rights)

### What We Can Share

✅ **Clearly Public**:
- GitHub repositories with public licenses
- Educational analysis projects
- Transparency demonstrations
- Official documentation
- Academic publications

❓ **Potentially Problematic**:
- Extracted from web interfaces
- API responses
- Reverse engineered
- Terms of service ambiguous

❌ **Definitely Not**:
- Trade secrets revealed
- Confidential information
- Private API responses
- Security vulnerabilities

### Our Sources

**Primary Sources (Public Repositories)**:
- https://github.com/dnnyngyen/kimi-k2.5-system-analysis
- https://github.com/dontriskit/awesome-ai-system-prompts

**Legal Basis for Collection**:
- Publicly accessible repositories
- No terms of service violations
- Educational/research use
- Attribution provided
- Transformation (organization, metadata)

---

## Educational Purpose

### Learning Objectives

This repository supports education in:

1. **AI Safety Engineering**
   - Understanding safety implementations
   - Comparing alignment approaches
   - Evaluating constraint methods

2. **System Architecture**
   - Prompt design patterns
   - Tool integration methods
   - Specialization techniques

3. **Transparent AI**
   - Democratizing AI knowledge
   - Enabling public discourse
   - Supporting policy discussions

4. **Research Methods**
   - Ethical data collection
   - Verification processes
   - Documentation standards

### Target Audiences

**Primary**: AI safety researchers, ML engineers, academics
**Secondary**: Policy makers, journalists, students, developers
**Not Targeted**: Those seeking to bypass safety or cause harm

### Curriculum Integration

This repository is designed for:

- **University Courses**: AI safety, ML engineering, ethics
- **Workshops**: Prompt engineering, AI architecture
- **Self-Study**: Developers learning about AI systems
- **Journalism**: Understanding AI capabilities and limits

---

## Transparency Principles

### Source Transparency

Every prompt includes:
- Exact source URL
- Original author/repository
- Extraction date
- Collection method
- Confidence level

This allows:
- Independent verification
- Historical tracking
- Responsibility attribution
- Error correction

### Method Transparency

We document:
- How prompts were obtained
- Verification processes
- Quality assurance steps
- Limitations and uncertainties

This enables:
- Reproducibility
- Methodological critique
- Process improvement
- Trust building

### Limitation Transparency

We are open about:
- What we don't know
- Confidence levels
- Methodological constraints
- Scope boundaries

This prevents:
- Overconfident claims
- Misuse due to misunderstanding
- Extrapolation beyond data
- Accountability gaps

---

## Contributor Responsibilities

### Before Contributing

**Check**:
- [ ] Source is legal and public
- [ ] No terms of service violations
- [ ] Educational value is clear
- [ ] Proper attribution possible
- [ ] No privacy violations

**Ask Yourself**:
- Would this cause harm if misused?
- Is the educational value clear?
- Am I following all applicable laws?
- Would I be comfortable with public scrutiny?

### During Contribution

**Document**:
- Complete source information
- Extraction method details
- Verification process
- Any ethical concerns

**Analyze**:
- Potential for misuse
- Safety implications
- Research significance
- Transparency benefits

### After Contributing

**Monitor**:
- How the contribution is used
- Community feedback
- Safety implications
- Calls for removal

**Respond**:
- To concerns and questions
- Investigation requests
- Removal requests (if valid)
- Improvement suggestions

---

## Community Standards

### Expected Behavior

- Respectful communication
- Thoughtful consideration of impacts
- Willingness to learn and improve
- Proactive safety thinking
- Constructive feedback

### Unacceptable Behavior

- Harassment or abuse
- Malicious use of prompts
- Disengagement from safety concerns
- Withholding known risks
- Attack-oriented applications

### Reporting Concerns

**To Report Issues**:

1. Open GitHub issue with "[Ethics Concern]" prefix
2. Email: ethics@agiterminal.example
3. Describe specific concern
4. Suggest resolution if possible

**We Respond To**:
- Misuse reports
- Privacy concerns
- Copyright issues
- Safety risks
- Legal compliance questions

### Conflict Resolution

**Process**:
1. Issue filed in good faith
2. Investigation by maintainers
3. Community discussion if needed
4. Decision with explanation
5. Appeal process available

**Possible Outcomes**:
- Content modification
- Removal of content
- Enhanced warnings
- Usage restrictions
- Community guidelines update

---

## Harm Prevention

### Proactive Measures

**In Collection**:
- Legal review of sources
- Educational framing
- Clear documentation
- Safety warnings

**In Distribution**:
- Responsible use guidelines
- Community monitoring
- Usage tracking
- Response procedures

**In Application**:
- Research context
- Educational purpose
- Safety emphasis
- Responsible handling

### Red Lines

We will remove content that:
- Is clearly illegal
- Has high misuse potential with low educational value
- Poses immediate safety risks
- Causes ongoing harm
- Violates fundamental ethical principles

### Ongoing Monitoring

**We Track**:
- How repository is referenced
- Any reported misuse
- Community feedback
- Safety research developments
- Legal/regulatory changes

**We Adapt**:
- Guidelines as needed
- Content based on concerns
- Methods based on best practices
- Policies based on experience

---

## Ethical Questions & Answers

### Q: Is collecting system prompts legal?

**A**: It depends on the source:
- **Public repositories**: Generally yes (MIT, Apache, etc.)
- **Web scraping**: In legal gray area, varies by terms of service
- **API extraction**: Usually violates ToS, not included here
- **Official docs**: Yes, if redistributable

**Our collection**: From public repositories only

### Q: Could this help people bypass AI safety?

**A**: Possibly, but we believe transparency benefits outweigh risks:
- Safety through understanding, not obscurity
- Public prompts enable research on defenses
- Transparency enables informed regulation
- Community can develop better safeguards

We include safety analysis and encourage responsible use.

### Q: Shouldn't AI companies keep prompts secret?

**A**: We disagree for several reasons:
- Public prompts enable safety research
- Transparency builds trust
- Users deserve to know how systems work
- Secret prompts still leak (as evidenced here)
- Public discourse needs accurate information

That said, we respect companies' legitimate IP concerns while advocating for appropriate transparency.

### Q: What if someone misuses these prompts?

**A**: We can't control misuse, but we:
- Frame everything for education
- Include prominent warnings
- Monitor for known harms
- Respond to abuse reports
- Emphasize safety in analysis

We believe educational value justifies collection given reasonable safeguards.

### Q: How do you handle removal requests?

**A**: We evaluate requests based on:
1. **Legal grounds** (copyright, etc.)
2. **Safety evidence** (documented misuse)
3. **Ethical concerns** (privacy, harm)
4. **Educational impact** (harm vs. benefit)

We err on the side of removal if legitimate concerns exist.

---

## Contact

For ethics questions:
- **GitHub Issues**: Use "[Ethics Question]" prefix
- **Email**: ethics@agiterminal.example
- **Escalation**: Contact maintainers directly

For urgent safety concerns:
- **Immediate Response**: Email safety@agiterminal.example
- **Include**: Specific issue, evidence, urgency

For legal matters:
- **Legal Contact**: legal@agiterminal.example
- **Include**: Legal basis, jurisdiction, timeframe

---

## Acknowledgments

We thank:
- AI safety researchers who study transparency
- Providers who publish prompts openly
- Community members who report concerns
- Developers who apply knowledge responsibly

Together we advance AI understanding while promoting safety and ethics.

---

*This document is reviewed quarterly and updated as needed. Last reviewed 2026-02-05.*
